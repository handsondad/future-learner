<!DOCTYPE html>
  <html>
    <head>
      <title>DeepSeek V3 详解</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\QXZ4FNN\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.8.15\crossnote\dependencies\katex\katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><html><head><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body></body></html>
    </head>
    <body for="html-export" >
      <div class="crossnote markdown-preview  "  >
      
<h1 id="deepseek-v3-详解">DeepSeek V3 详解 </h1>
<p>以下是通过 DeepSeek Chat来理解其原理，我们可以基于它自身来学习。</p>
<p align="center">
  <img src="file:///c:\Xiuqin\Code\DeepSeek\DeepSeek-V3-main\figures\DeepSeek V3 Architecture.png">
</p>
<h2 id="关于-transformer">关于 Transformer </h2>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer 模型，包含位置嵌入、多层 transformer 块和输出投影层。

    Attributes:
        max_seq_len (int): 模型支持的最大序列长度。
        embed (nn.Module): 输入 token 的嵌入层。
        layers (torch.nn.ModuleList): 包含所有 transformer 块的模块列表。
        norm (nn.Module): 所有 transformer 块后的层归一化。
        head (nn.Module): 输出投影层，映射到词表大小。
        freqs_cis (torch.Tensor): 预计算的旋转位置嵌入的复指数值。
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化 Transformer 模型。

        Args:
            args (ModelArgs): 包含模型参数的 ModelArgs 对象。
        """</span>
        <span class="token keyword keyword-global">global</span> world_size<span class="token punctuation">,</span> rank
        <span class="token comment"># 获取分布式环境中的 world_size 和 rank</span>
        world_size <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> dist<span class="token punctuation">.</span>is_initialized<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-else">else</span> <span class="token number">1</span>
        rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> dist<span class="token punctuation">.</span>is_initialized<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-else">else</span> <span class="token number">0</span>
        <span class="token comment"># 根据模型参数设置线性层的精度</span>
        Linear<span class="token punctuation">.</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float8_e4m3fn <span class="token keyword keyword-if">if</span> args<span class="token punctuation">.</span>dtype <span class="token operator">==</span> <span class="token string">"fp8"</span> <span class="token keyword keyword-else">else</span> torch<span class="token punctuation">.</span>bfloat16
        <span class="token comment"># 调用父类初始化函数</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 设置最大序列长度</span>
        self<span class="token punctuation">.</span>max_seq_len <span class="token operator">=</span> args<span class="token punctuation">.</span>max_seq_len
        <span class="token comment"># 初始化嵌入层</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> ParallelEmbedding<span class="token punctuation">(</span>args<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化多层 transformer 块</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-for">for</span> layer_id <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Block<span class="token punctuation">(</span>layer_id<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化层归一化</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化输出投影层</span>
        self<span class="token punctuation">.</span>head <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 注册缓冲区，存储预计算的旋转位置嵌入的复指数值</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"freqs_cis"</span><span class="token punctuation">,</span> precompute_freqs_cis<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Transformer 模型的前向传播。

        Args:
            tokens (torch.Tensor): 输入 token ID 张量，形状为 (batch_size, seq_len)。
            start_pos (int, optional): 旋转位置嵌入的起始位置，默认为 0。

        Returns:
            torch.Tensor: 形状为 (batch_size, vocab_size) 的 logits 张量。
        """</span>
        <span class="token comment"># 获取输入序列长度</span>
        seqlen <span class="token operator">=</span> tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 将输入 token 转换为嵌入向量</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
        <span class="token comment"># 获取从 start_pos 开始的旋转位置嵌入的复指数值</span>
        freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">[</span>start_pos<span class="token punctuation">:</span>start_pos<span class="token operator">+</span>seqlen<span class="token punctuation">]</span>
        <span class="token comment"># 如果序列长度大于 1，生成掩码（上三角为负无穷，防止未来信息泄漏）</span>
        mask <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword keyword-if">if</span> seqlen <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> seqlen<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>triu_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 逐层通过 transformer 块</span>
        <span class="token keyword keyword-for">for</span> layer <span class="token keyword keyword-in">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            h <span class="token operator">=</span> layer<span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token comment"># 对最后一层的输出进行归一化，并取最后一个位置的向量</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 通过输出投影层得到 logits</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        <span class="token comment"># 如果启用了分布式，收集所有 GPU 的 logits</span>
        <span class="token keyword keyword-if">if</span> world_size <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            all_logits <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>logits<span class="token punctuation">)</span> <span class="token keyword keyword-for">for</span> _ <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">]</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>all_logits<span class="token punctuation">,</span> logits<span class="token punctuation">)</span>
            logits <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>all_logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 返回最终的 logits</span>
        <span class="token keyword keyword-return">return</span> logits

<span class="token keyword keyword-class">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer 块，结合了注意力层和前馈网络层。

    Attributes:
        attn (nn.Module): 注意力层 (MLA)。
        ffn (nn.Module): 前馈网络 (MLP 或 MoE)。
        attn_norm (nn.Module): 注意力层的层归一化。
        ffn_norm (nn.Module): 前馈网络的层归一化。
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_id<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化 Transformer 块。

        Args:
            layer_id (int): Transformer 中的层索引。
            args (ModelArgs): 包含块参数的 ModelArgs 对象。
        """</span>
        <span class="token comment"># 调用父类初始化函数</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化注意力层 (MLA)</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> MLA<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 根据层索引选择前馈网络：前几层使用 MLP，后续层使用 MoE</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> MLP<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>inter_dim<span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> layer_id <span class="token operator">&lt;</span> args<span class="token punctuation">.</span>n_dense_layers <span class="token keyword keyword-else">else</span> MoE<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 初始化注意力层的层归一化</span>
        self<span class="token punctuation">.</span>attn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化前馈网络的层归一化</span>
        self<span class="token punctuation">.</span>ffn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Transformer 块的前向传播。

        Args:
            x (torch.Tensor): 输入张量。
            start_pos (int): 序列中的起始位置。
            freqs_cis (torch.Tensor): 预计算的旋转位置嵌入的复指数值。
            mask (Optional[torch.Tensor]): 注意力掩码张量，用于排除某些位置。

        Returns:
            torch.Tensor: 块计算后的输出张量。
        """</span>
        <span class="token comment"># 残差连接：先对输入进行层归一化，然后通过注意力层</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token comment"># 残差连接：先对输入进行层归一化，然后通过前馈网络</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ffn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 返回输出张量</span>
        <span class="token keyword keyword-return">return</span> x

</code></pre><h2 id="关于-mla">关于 MLA </h2>
<p>Multi-Head Latent Attention（多头潜在注意力）<strong>是一种改进的注意力机制，通常用于深度学习和自然语言处理（NLP）任务中。它结合了</strong>多头注意力机制（Multi-Head Attention）<strong>和</strong>潜在表示（Latent Representation）**的思想，旨在更有效地捕获输入数据中的复杂依赖关系和潜在结构。</p>
<h3 id="概念和介绍">概念和介绍 </h3>
<p>以下是 Multi-Head Latent Attention 的核心概念和介绍：</p>
<h4 id="多头注意力机制multi-head-attention"><strong>多头注意力机制（Multi-Head Attention）</strong> </h4>
<ul>
<li>多头注意力是 Transformer 模型的核心组件，由 Vaswani 等人在 2017 年提出。</li>
<li>通过将输入数据映射到多个子空间（即“头”），多头注意力可以并行地捕获输入的不同特征。</li>
<li>每个头独立地计算注意力权重，然后将结果拼接在一起进行融合。</li>
</ul>
<h4 id="潜在表示latent-representation"><strong>潜在表示（Latent Representation）</strong> </h4>
<ul>
<li>潜在表示是指数据在隐空间中的抽象表示，通常通过深度学习模型（如自编码器或生成模型）学习得到。</li>
<li>潜在表示可以捕捉数据的高层次语义特征，并减少噪声。</li>
</ul>
<h4 id="multi-head-latent-attention-的核心思想"><strong>Multi-Head Latent Attention 的核心思想</strong> </h4>
<p>Multi-Head Latent Attention 在传统的多头注意力机制的基础上，引入了潜在表示的思想。具体来说：</p>
<ul>
<li>它通过将输入数据映射到一个潜在空间（latent space）来提取更高层次的语义特征。</li>
<li>在潜在空间中计算多头注意力，从而更有效地捕获输入数据中的全局和局部依赖关系。</li>
<li>这种方法可以提高模型对复杂模式的建模能力，同时减少计算复杂度。</li>
</ul>
<h4 id="工作原理"><strong>工作原理</strong> </h4>
<p>Multi-Head Latent Attention 的工作流程通常包括以下步骤：</p>
<ol>
<li><strong>输入映射</strong>：
<ul>
<li>将输入数据（如词嵌入或特征向量）通过一个非线性变换（如全连接层或卷积层）映射到潜在空间。</li>
</ul>
</li>
<li><strong>多头注意力计算</strong>：
<ul>
<li>在潜在空间中，将输入数据分为多个“头”，每个头独立地计算注意力权重。</li>
<li>使用点积注意力（Scaled Dot-Product Attention）或更复杂的注意力机制计算每个头的权重。</li>
</ul>
</li>
<li><strong>结果融合</strong>：
<ul>
<li>将所有头的输出拼接在一起，并通过一个线性变换将其映射回原始空间。</li>
</ul>
</li>
<li><strong>输出</strong>：
<ul>
<li>输出融合后的结果，可以用于下游任务（如分类、生成或预测）。</li>
</ul>
</li>
</ol>
<h4 id="优势"><strong>优势</strong> </h4>
<ul>
<li><strong>更好的特征提取</strong>：通过潜在表示，能够捕捉输入数据的高层次语义特征。</li>
<li><strong>更高的效率</strong>：在潜在空间中计算注意力可以减少计算复杂度，尤其是在处理高维数据时。</li>
<li><strong>更强的建模能力</strong>：多头机制和潜在表示的结合，使得模型能够同时捕获全局和局部的依赖关系。</li>
</ul>
<h4 id="应用场景"><strong>应用场景</strong> </h4>
<p>Multi-Head Latent Attention 可以广泛应用于以下领域：</p>
<ul>
<li><strong>自然语言处理（NLP）</strong>：如文本分类、机器翻译、问答系统等。</li>
<li><strong>计算机视觉（CV）</strong>：如图像分类、目标检测、生成对抗网络（GAN）等。</li>
<li><strong>多模态学习</strong>：如图文对齐、视频理解等。</li>
<li><strong>时间序列分析</strong>：如股票预测、语音识别等。</li>
</ul>
<h4 id="与传统多头注意力的区别"><strong>与传统多头注意力的区别</strong> </h4>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>传统多头注意力</strong></th>
<th><strong>多头潜在注意力</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>输入空间</strong></td>
<td>原始输入空间</td>
<td>潜在空间</td>
</tr>
<tr>
<td><strong>特征提取能力</strong></td>
<td>直接捕获输入特征</td>
<td>捕获高层次语义特征</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>较高</td>
<td>可能降低（取决于潜在空间的维度）</td>
</tr>
<tr>
<td><strong>建模能力</strong></td>
<td>有限于原始输入空间</td>
<td>更强，适合复杂依赖关系</td>
</tr>
</tbody>
</table>
<h4 id="相关研究"><strong>相关研究</strong> </h4>
<p>Multi-Head Latent Attention 的灵感来自以下研究领域：</p>
<ul>
<li><strong>Transformer 模型</strong>：Vaswani 等人在 2017 年提出的注意力机制。</li>
<li><strong>变分自编码器（VAE）</strong>：通过潜在表示学习数据分布。</li>
<li><strong>生成对抗网络（GAN）</strong>：在潜在空间中生成高质量样本。</li>
</ul>
<h4 id="总结">总结 </h4>
<p>Multi-Head Latent Attention 是一种结合多头注意力和潜在表示的先进注意力机制，能够更有效地建模复杂数据中的依赖关系。它在 NLP、CV 和多模态学习等领域具有广泛的应用前景，是深度学习研究中的一个重要方向。</p>
<h3 id="代码详解">代码详解 </h3>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">MLA</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Multi-Headed Attention Layer (MLA).

    Attributes:
        dim (int): Dimensionality of the input features.
        n_heads (int): Number of attention heads.
        n_local_heads (int): Number of local attention heads for distributed systems.
        q_lora_rank (int): Rank for low-rank query projection.
        kv_lora_rank (int): Rank for low-rank key/value projection.
        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.
        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.
        qk_head_dim (int): Total dimensionality of query/key projections.
        v_head_dim (int): Dimensionality of value projections.
        softmax_scale (float): Scaling factor for softmax in attention computation.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 从args中获取模型的维度、注意力头数等参数</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim  <span class="token comment"># 模型的维度</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads  <span class="token comment"># 总的注意力头数</span>
        self<span class="token punctuation">.</span>n_local_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token operator">//</span> world_size  <span class="token comment"># 本地注意力头数，world_size可能是分布式训练中的进程数</span>
        self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">=</span> args<span class="token punctuation">.</span>q_lora_rank  <span class="token comment"># 查询（Query）LoRA的秩</span>
        self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">=</span> args<span class="token punctuation">.</span>kv_lora_rank  <span class="token comment"># 键值（Key-Value）LoRA的秩</span>
        self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_nope_head_dim  <span class="token comment"># 不使用位置编码的注意力头的维度</span>
        self<span class="token punctuation">.</span>qk_rope_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_rope_head_dim  <span class="token comment"># 使用位置编码的注意力头的维度</span>
        self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> args<span class="token punctuation">.</span>qk_rope_head_dim  <span class="token comment"># 总注意力头的维度</span>
        self<span class="token punctuation">.</span>v_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>v_head_dim  <span class="token comment"># 值（Value）头的维度</span>

        <span class="token comment"># 根据q_lora_rank的值，选择不同的查询权重初始化方式</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果q_lora_rank为0，直接使用ColumnParallelLinear初始化查询权重</span>
            self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 否则，使用LoRA结构初始化查询权重</span>
            self<span class="token punctuation">.</span>wq_a <span class="token operator">=</span> Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>  <span class="token comment"># LoRA的第一层</span>
            self<span class="token punctuation">.</span>q_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>  <span class="token comment"># 对LoRA的输出进行归一化</span>
            self<span class="token punctuation">.</span>wq_b <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>  <span class="token comment"># LoRA的第二层</span>

        <span class="token comment"># 初始化键值（Key-Value）权重</span>
        self<span class="token punctuation">.</span>wkv_a <span class="token operator">=</span> Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">+</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">)</span>  <span class="token comment"># LoRA的第一层</span>
        self<span class="token punctuation">.</span>kv_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span>  <span class="token comment"># 对LoRA的输出进行归一化</span>
        self<span class="token punctuation">.</span>wkv_b <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># LoRA的第二层</span>

        <span class="token comment"># 初始化输出权重</span>
        self<span class="token punctuation">.</span>wo <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>

        <span class="token comment"># 设置softmax的缩放因子</span>
        self<span class="token punctuation">.</span>softmax_scale <span class="token operator">=</span> self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>

        <span class="token comment"># 如果最大序列长度大于原始序列长度，调整softmax的缩放因子</span>
        <span class="token keyword keyword-if">if</span> args<span class="token punctuation">.</span>max_seq_len <span class="token operator">&gt;</span> args<span class="token punctuation">.</span>original_seq_len<span class="token punctuation">:</span>
            mscale <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token operator">*</span> args<span class="token punctuation">.</span>mscale <span class="token operator">*</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>args<span class="token punctuation">.</span>rope_factor<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span>
            self<span class="token punctuation">.</span>softmax_scale <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax_scale <span class="token operator">*</span> mscale <span class="token operator">*</span> mscale

        <span class="token comment"># 根据注意力实现方式的不同，初始化不同的缓存</span>
        <span class="token keyword keyword-if">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果是"naive"实现，初始化键和值的缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"k_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"v_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 否则，初始化键值和位置编码的缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"kv_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"pe_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for the Multi-Headed Attention Layer (MLA).

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).
            start_pos (int): Starting position in the sequence for caching.
            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.
            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.

        Returns:
            torch.Tensor: Output tensor with the same shape as the input.
        """</span>
        <span class="token comment"># 获取输入张量的 batch size 和序列长度</span>
        bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 计算结束位置</span>
        end_pos <span class="token operator">=</span> start_pos <span class="token operator">+</span> seqlen

        <span class="token comment"># 根据 q_lora_rank 的值选择不同的查询（Query）计算方式</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果 q_lora_rank 为 0，直接使用 wq 计算查询</span>
            q <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 否则，使用 LoRA 结构计算查询</span>
            q <span class="token operator">=</span> self<span class="token punctuation">.</span>wq_b<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>wq_a<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 调整查询张量的形状，以适应多头注意力机制</span>
        q <span class="token operator">=</span> q<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
        <span class="token comment"># 将查询张量分为不使用位置编码和使用位置编码的部分</span>
        q_nope<span class="token punctuation">,</span> q_pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对使用位置编码的部分应用旋转嵌入</span>
        q_pe <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>q_pe<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">)</span>

        <span class="token comment"># 计算键值（Key-Value）张量</span>
        kv <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 将键值张量分为 LoRA 部分和位置编码部分</span>
        kv<span class="token punctuation">,</span> k_pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>kv<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对键值的位置编码部分应用旋转嵌入</span>
        k_pe <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>k_pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> freqs_cis<span class="token punctuation">)</span>

        <span class="token comment"># 根据注意力实现方式选择不同的计算逻辑</span>
        <span class="token keyword keyword-if">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果不使用 LoRA 优化，直接拼接查询的不使用位置编码部分和使用位置编码部分</span>
            q <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>q_nope<span class="token punctuation">,</span> q_pe<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 对键值张量应用归一化并通过 wkv_b 进行线性变换</span>
            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_norm<span class="token punctuation">(</span>kv<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># 调整键值张量的形状，以适应多头注意力机制</span>
            kv <span class="token operator">=</span> kv<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>
            <span class="token comment"># 将键值张量分为键的不使用位置编码部分和值部分</span>
            k_nope<span class="token punctuation">,</span> v <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>kv<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 将键的不使用位置编码部分和位置编码部分拼接起来</span>
            k <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_nope<span class="token punctuation">,</span> k_pe<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 更新缓存中的键和值</span>
            self<span class="token punctuation">.</span>k_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> k
            self<span class="token punctuation">.</span>v_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> v
            <span class="token comment"># 计算注意力分数，通过矩阵乘法（einsum）计算查询和键的点积，并乘以 softmax 缩放因子</span>
            scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshd,bthd-&gt;bsht"</span><span class="token punctuation">,</span> q<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>softmax_scale
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果使用 LoRA 优化，获取 wkv_b 的权重并进行反量化（如果 scale 存在）</span>
            wkv_b <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>weight <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>scale <span class="token keyword keyword-is">is</span> <span class="token boolean">None</span> <span class="token keyword keyword-else">else</span> weight_dequant<span class="token punctuation">(</span>self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>scale<span class="token punctuation">,</span> block_size<span class="token punctuation">)</span>
            <span class="token comment"># 调整 wkv_b 的形状，以适应多头注意力机制</span>
            wkv_b <span class="token operator">=</span> wkv_b<span class="token punctuation">.</span>view<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span>
            <span class="token comment"># 计算不使用位置编码部分的查询与 wkv_b 的线性变换结果</span>
            q_nope <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshd,hdc-&gt;bshc"</span><span class="token punctuation">,</span> q_nope<span class="token punctuation">,</span> wkv_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment"># 更新缓存中的键值部分和位置编码部分</span>
            self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>kv_norm<span class="token punctuation">(</span>kv<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>pe_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> k_pe<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token comment"># 计算注意力分数，分别计算不使用位置编码部分和位置编码部分的分数，然后相加并乘以 softmax 缩放因子</span>
            scores <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshc,btc-&gt;bsht"</span><span class="token punctuation">,</span> q_nope<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span>
                    torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshr,btr-&gt;bsht"</span><span class="token punctuation">,</span> q_pe<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pe_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>softmax_scale

        <span class="token comment"># 如果提供了 mask 张量，将其添加到注意力分数中（用于屏蔽某些位置）</span>
        <span class="token keyword keyword-if">if</span> mask <span class="token keyword keyword-is">is</span> <span class="token keyword keyword-not">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">+=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># 对注意力分数进行 softmax 归一化，以确保分数在 0 到 1 之间，并转换为输入张量的数据类型</span>
        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 根据注意力实现方式选择不同的输出计算逻辑</span>
        <span class="token keyword keyword-if">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果不使用 LoRA 优化，直接通过矩阵乘法计算输出</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bsht,bthd-&gt;bshd"</span><span class="token punctuation">,</span> scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果使用 LoRA 优化，先计算中间结果，然后通过 wkv_b 进行线性变换</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bsht,btc-&gt;bshc"</span><span class="token punctuation">,</span> scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshc,hdc-&gt;bshd"</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> wkv_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 将多头注意力的输出展平，并通过 wo 进行线性变换，得到最终输出</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>wo<span class="token punctuation">(</span>x<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 返回输出张量</span>
        <span class="token keyword keyword-return">return</span> x

</code></pre><p>这段代码定义了一个 <strong>Multi-Headed Attention Layer (MLA)</strong>，用于处理多注意力机制的任务。以下是对代码的总结，方便理解：</p>
<h4 id="核心目标"><strong>核心目标</strong> </h4>
<ul>
<li><strong>MLA</strong> 是一个多头注意力层，用于在深度神经网络中实现注意力机制，特别是在处理序列数据时。</li>
<li>它支持低秩投影（LoRA）和位置编码（Rotary Embedding），并能在分布式系统中处理局部注意力。</li>
</ul>
<h4 id="主要功能与结构"><strong>主要功能与结构</strong> </h4>
<ol>
<li>
<p><strong>初始化 (<code>__init__</code>)</strong></p>
<ul>
<li><strong>参数配置</strong>：根据 <code>ModelArgs</code> 配置维度和注意力头的数量。</li>
<li><strong>低秩投影 (<code>q_lora_rank</code>, <code>kv_lora_rank</code>)</strong>：如果启用，使用低秩投影来降低计算复杂度。</li>
<li><strong>线性层</strong>：
<ul>
<li><code>wq</code>, <code>wkv_a</code>, <code>wkv_b</code>, <code>wo</code>：分别用于查询（Query）、键/值（Key/Value）和输出（Output）的线性变换。</li>
</ul>
</li>
<li><strong>缓存机制</strong>：根据 <code>attn_impl</code> 的不同实现方式，初始化键/值缓存（<code>k_cache</code>, <code>v_cache</code> 或 <code>kv_cache</code>）和位置编码缓存（<code>pe_cache</code>）。</li>
</ul>
</li>
<li>
<p><strong>前向传播 (<code>forward</code>)</strong></p>
<ul>
<li><strong>输入</strong>：
<ul>
<li><code>x</code>：输入张量，形状为 <code>(batch_size, seq_len, dim)</code>。</li>
<li><code>start_pos</code>：序列中的起始位置，用于缓存。</li>
<li><code>freqs_cis</code>：预计算的旋转位置编码（Rotary Embedding）值。</li>
<li><code>mask</code>：可选掩码张量，用于屏蔽某些位置的注意力。</li>
</ul>
</li>
<li><strong>核心步骤</strong>：
<ol>
<li><strong>查询（Query）</strong>：通过 <code>wq</code> 或低秩投影 (<code>wq_a</code>, <code>wq_b</code>) 计算查询向量。</li>
<li><strong>键/值（Key/Value）</strong>：通过 <code>wkv_a</code> 和 <code>wkv_b</code> 计算键和值向量，并应用位置编码。</li>
<li><strong>缓存更新</strong>：将计算得到的键/值和位置编码存入缓存。</li>
<li><strong>注意力分数计算</strong>：通过点积计算注意力分数，并应用缩放因子和掩码。</li>
<li><strong>输出计算</strong>：根据注意力分数对值进行加权求和，最终通过 <code>wo</code> 线性层输出结果。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>实现细节</strong></p>
<ul>
<li><strong>Naive 实现</strong>：直接计算完整的键/值缓存，适用于较短序列。</li>
<li><strong>高效实现</strong>：通过低秩投影和缓存机制优化计算，适用于长序列和分布式系统。</li>
</ul>
</li>
</ol>
<h4 id="关键特点"><strong>关键特点</strong> </h4>
<ul>
<li><strong>低秩投影</strong>：通过 <code>q_lora_rank</code> 和 <code>kv_lora_rank</code> 支持低秩投影，减少计算开销。</li>
<li><strong>位置编码</strong>：使用旋转位置编码 (<code>Rotary Embedding</code>) 增强模型对序列位置信息的捕捉能力。</li>
<li><strong>缓存机制</strong>：在 <code>forward</code> 过程中缓存键/值和位置编码，避免重复计算。</li>
<li><strong>分布式支持</strong>：通过 <code>n_local_heads</code> 支持分布式系统中的局部注意力计算。</li>
</ul>
<h4 id="总结-1"><strong>总结</strong> </h4>
<p>这段代码实现了一个高效的多头注意力层，支持低秩投影、位置编码和分布式计算。通过灵活的缓存机制和优化实现，它能够处理长短不一的序列数据，并适用于多种硬件和分布式环境。</p>
<h2 id="关于-rope">关于 RoPE </h2>
<p>RoPE，全称为“Rotary Positional Embedding”（旋转位置嵌入），是自然语言处理领域中，特别是在处理序列数据的Transformer模型中使用的一种创新的位置编码技术。它由Alexander H. Wu、Kaiwen Wu等人在2021年的论文《RoFormer: Enhanced Transformer with Rotary Positional Encoding for Long-Sequence Text Summarization》中首次提出。RoPE的提出是为了改进标准的固定位置编码方法，以解决长序列文本处理中的位置依赖性问题，同时减缓模型在序列长度变化时的表现下降。</p>
<h3 id="概念和介绍-1">概念和介绍 </h3>
<h4 id="核心思想">核心思想 </h4>
<p>传统的Transformer模型通常使用Sinusoidal位置编码（如Bert），将位置信息嵌入到模型输入中。然而，这种方法存在局限性，尤其是在处理长度可变的输入序列时，固定的编码可能难以适应序列长度的变化，影响模型的泛化能力。</p>
<p>RoPE通过引入旋转操作，将位置信息以一种动态且参数化的方式嵌入到Transformer模型的注意力机制中。具体而言，RoPE对查询和键（Query和Key）向量的位置信息进行编码，然后通过矩阵旋转来更新这些向量，而不是直接加法或乘法操作。这种方法保留了绝对位置信息，同时增强了模型对相对位置的敏感性，从而提高了模型处理长序列数据的能力。</p>
<h4 id="运作机制">运作机制 </h4>
<p>在RoPE中，每个位置的Query和Key向量首先通过相应的旋转矩阵进行编码。旋转矩阵是基于正弦和余弦函数构建的，与位置相关联，允许模型捕捉相对位置关系。在计算注意力机制时，这些旋转后的向量会参与到注意力权重的计算中，从而影响最终的注意力分布和价值（Value）向量的加权和。</p>
<h4 id="主要优势">主要优势 </h4>
<ul>
<li><strong>位置不变性</strong>：RoPE使得模型在处理不同长度的序列时能够更好地保持一致性，因为它不依赖于特定的序列长度。</li>
<li><strong>相对位置编码</strong>：通过更新位置信息的编码方式，RoPE增强了模型对相对位置的感知，有助于处理长距离依赖。</li>
<li><strong>参数高效性</strong>：与固定位置编码相比，RoPE的引入通常不需要增加额外的模型参数，有助于控制模型的规模和复杂度。</li>
<li><strong>序列长度独立性</strong>：RoPE的动态编码机制使得模型在处理变量长度的输入时更加灵活，无需为每一种可能的序列长度预计算位置编码。</li>
</ul>
<p>RoPE自提出以来，已在多种NLP任务中显示出其有效性和实用性，包括文本摘要、对话系统、机器阅读理解等，为处理长序列数据提供了新的视角和解决方案。</p>
<h3 id="代码详解-1">代码详解 </h3>
<p>理解下RoPE (Rotary Positional Embedding)的代码实现：</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">apply_rotary_emb</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Applies rotary positional embeddings to the input tensor.

    Args:
        x (torch.Tensor): Input tensor with positional embeddings to be applied.
        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.

    Returns:
        torch.Tensor: Tensor with rotary embeddings applied.
    """</span>
    dtype <span class="token operator">=</span> x<span class="token punctuation">.</span>dtype
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    freqs_cis <span class="token operator">=</span> freqs_cis<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>x <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-return">return</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>
</code></pre><p>这段代码实现了RoPE (Rotary Positional Embedding)在给定输入张量上的应用，它是处理序列数据时用于增强模型对相对位置的理解的一种技术，特别在Transformer架构中常见。下面是对代码的逐行详细解释：</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">apply_rotary_emb</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
</code></pre><p>定义了函数 <code>apply_rotary_emb</code>，它接受两个参数：张量 <code>x</code>（输入序列），以及 <code>freqs_cis</code>（用于旋转的位置编码张量）。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>dtype <span class="token operator">=</span> x<span class="token punctuation">.</span>dtype
</code></pre><p>存储了输入张量 <code>x</code> 的原始数据类型，这通常是为了在转换类型进行计算后，能将结果转换回原始类型，确保与模型的其他部分兼容。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_complex<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><p>这一行代码做了几件事：</p>
<ol>
<li>首先将张量 <code>x</code> 转换为浮点数类型，这是因为复数张量在PyTorch中需要以浮点数形式操作。</li>
<li>然后，对 <code>x</code> 的形状进行重塑，将最后一维的元素通过 <code>-1, 2</code> 重塑为复数的实部和虚部。这相当于将最后一维的一对元素解释为一个复数，即最后一维的每个元素被拆分为实部和虚部。</li>
<li>最终，使用 <code>torch.view_as_complex</code> 将重塑的张量转换为复数张量。</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>freqs_cis <span class="token operator">=</span> freqs_cis<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><p>这里，<code>freqs_cis</code> 张量被重塑，以匹配张量 <code>x</code> 的各个维度。这是为了后续的元素级乘法做准备，确保维度对齐。<code>freqs_cis</code> 张量的形状调整，使得它可以在批量处理和头数维度上进行广播。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>view_as_real<span class="token punctuation">(</span>x <span class="token operator">*</span> freqs_cis<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
</code></pre><p>执行旋转操作：</p>
<ol>
<li><code>x * freqs_cis</code> 在复数域中进行元素级乘法。由于 <code>x</code> 和 <code>freqs_cis</code> 都被视为复数张量，这实质上是进行了复数乘法，实现了旋转操作。</li>
<li>然后，使用 <code>torch.view_as_real</code> 将乘法结果，即复数张量转换回实数张量表示。</li>
<li>最后，使用 <code>.flatten(3)</code> 将第四维及之后的维度展平，恢复原始的张量结构，除了将旋转操作中最后两个维度的“2”及之前由旋转引入的维度回缩到原本最后一维的位置。这样，结果张量的形状将与输入张量 <code>x</code> 相匹配，只除了在旋转操作中被“压缩”再“扩展”的维度部分。</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-return">return</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>
</code></pre><p>在进行完所有计算后，结果张量 <code>y</code> 被转换回与输入张量 <code>x</code> 相同的数据类型，然后返回。</p>
<p>总之，这段代码的核心目的是通过一个旋转操作应用位置编码。这一旋转操作有助于模型在处理长序列或需要对相对位置敏感的任务时，更好地捕获序列中元素间的依赖关系。</p>
<h3 id="理解下复数乘法的数学本质">理解下复数乘法的数学本质 </h3>
<p>在解释为什么<code>x * freqs_cis</code>在复数域中的元素级乘法能实现旋转操作之前，我们首先需要理解复数乘法的数学本质以及其如何与旋转操作关联。</p>
<h4 id="复数乘法的本质">复数乘法的本质 </h4>
<p>复数乘法的本质在于它不仅改变了复数的“长度”（模），还改变了它的“角度”（幅角）。数学上，如果两个复数分别表示为<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>=</mo><msub><mi>r</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>cos</mi><mo>⁡</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>+</mo><mi>i</mi><mi>sin</mi><mo>⁡</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z_1 = r_1(\cos\theta_1 + i\sin\theta_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mop">cos</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">sin</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><br>
和<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mn>2</mn></msub><mo>=</mo><msub><mi>r</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>cos</mi><mo>⁡</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>+</mo><mi>i</mi><mi>sin</mi><mo>⁡</mo><msub><mi>θ</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z_2 = r_2(\cos\theta_2 + i\sin\theta_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mop">cos</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">sin</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><br>
其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">r_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">r_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是它们的模，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\theta_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\theta_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是它们的幅角，那么它们的乘积<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><msub><mi>z</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">z_1z_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>可以表示为<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><msub><mi>z</mi><mn>2</mn></msub><mo>=</mo><msub><mi>r</mi><mn>1</mn></msub><msub><mi>r</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>+</mo><msub><mi>θ</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>i</mi><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>+</mo><msub><mi>θ</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z_1z_2 = r_1r_2(\cos(\theta_1+\theta_2) + i\sin(\theta_1+\theta_2))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span><br>
这表明复数乘法实际上结合了两者的“长度”乘积（模的乘积）和两个复数的角度相加。</p>
<h4 id="旋转操作与复数乘法">旋转操作与复数乘法 </h4>
<p>在二维平面上，一个复数可以被视作一个向量，它的实部和虚部分别对应于向量在x轴和y轴上的分量。从几何角度来看，复数的幅角则代表了这个向量与正x轴的夹角。因此，当我们乘以一个模为1（即“长度”为1）的复数时，实际上是在原地旋转这个向量而不改变其长度。</p>
<h4 id="rope中的旋转操作">RoPE中的旋转操作 </h4>
<p>在RoPE中使用的<code>freqs_cis</code>实际上是基于位置的复数值，这些值是周期函数（余弦和正弦）的输出，构造为<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><msub><mi>s</mi><mrow><mi>c</mi><mi>i</mi><mi>s</mi></mrow></msub><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>ω</mi><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>i</mi><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>ω</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">freqs_{cis}[t] = \cos(\omega t) + i\sin(\omega t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">t</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span><br>
其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>是位置索引，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span></span>是与频率相关的参数。由于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><msub><mi>s</mi><mrow><mi>c</mi><mi>i</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">freqs_{cis}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的模为1，当其与复数表示的向量（即变换后的<code>x</code>）相乘时，实际上是在对向量进行旋转。</p>
<h4 id="rope中的应用">RoPE中的应用 </h4>
<p>当输入张量<code>x</code>通过某些方法转换为复数表示，并与<code>freqs_cis</code>进行元素级乘法时，每一位置上的向量都会经历一个基于位置的独立旋转。这种旋转保留了向量的模不变，同时调整了它的方向，反映了时间（位置）信息。由于旋转的角度与位置索引直接相关，这种操作本质上就编码了位置信息到查询（Query）和键（Key）向量中，有助于模型学习序列中不同位置元素之间的相对关系。</p>
<p>因此，通过复数乘法，<code>x * freqs_cis</code>实际上实现了基于位置的旋转操作，这是一种非常巧妙的方法来显式地将位置信息嵌入到Transformer的注意力机制中，特别适合处理如长序列文本或语音数据等任务。</p>
<h2 id="关于-deepseekmoe">关于 DeepSeekMoE </h2>
<h3 id="deepseekmoe-简介"><strong>DeepSeekMoE 简介</strong> </h3>
<p><strong>DeepSeekMoE</strong> 是一种基于 <strong>Mixture of Experts (MoE)</strong> 的深度学习架构，专门设计用于高效处理大规模任务，尤其是在计算资源有限的情况下。MoE 的核心思想是将模型分解为多个专家（Experts），每个专家负责处理特定的子任务，而门控机制（Gating Network）则动态地决定如何组合这些专家的输出。DeepSeekMoE 结合了 MoE 的优势，同时通过一系列优化技术提高了模型的效率和性能。</p>
<p align="center">
  <img src="file:///c:\Xiuqin\Code\DeepSeek\DeepSeek-V3-main\figures\MoE Layer.png">
</p>
<h4 id="核心设计理念"><strong>核心设计理念</strong> </h4>
<ol>
<li>
<p><strong>Mixture of Experts (MoE)</strong>:</p>
<ul>
<li>模型由多个专家组成，每个专家是一个小的神经网络。</li>
<li>门控机制根据输入动态分配权重，决定哪些专家参与计算。</li>
</ul>
</li>
<li>
<p><strong>动态路由（Dynamic Routing）</strong>:</p>
<ul>
<li>输入数据通过门控网络分配，只有部分专家被激活，从而减少计算量。</li>
<li>这种稀疏激活机制使得模型能够扩展到更大的规模，同时保持高效性。</li>
</ul>
</li>
<li>
<p><strong>高效训练与推理</strong>:</p>
<ul>
<li>通过分布式训练和参数共享技术，优化大规模模型的训练效率。</li>
<li>支持低资源环境下的推理，例如边缘计算设备。</li>
</ul>
</li>
</ol>
<h4 id="关键技术特点"><strong>关键技术特点</strong> </h4>
<ol>
<li>
<p><strong>稀疏激活</strong>:</p>
<ul>
<li>只有少数专家被激活，减少计算开销。</li>
<li>通过门控机制实现动态选择，适应不同的输入数据。</li>
</ul>
</li>
<li>
<p><strong>参数共享与专家专业化</strong>:</p>
<ul>
<li>专家之间可以共享部分参数，减少模型的总体参数量。</li>
<li>每个专家专注于特定的任务或特征，提升模型的表达能力。</li>
</ul>
</li>
<li>
<p><strong>分布式训练</strong>:</p>
<ul>
<li>支持多 GPU 或多节点训练，适应大规模模型的需求。</li>
<li>通过高效的通信机制减少分布式训练的开销。</li>
</ul>
</li>
<li>
<p><strong>自适应门控</strong>:</p>
<ul>
<li>门控网络能够根据输入数据的特性动态调整专家的权重。</li>
<li>支持多种门控策略，例如 Top-K 选择或软注意力机制。</li>
</ul>
</li>
<li>
<p><strong>资源优化</strong>:</p>
<ul>
<li>针对低资源环境（如移动设备或嵌入式设备）进行优化，降低内存和计算需求。</li>
</ul>
</li>
</ol>
<h4 id="优势-1"><strong>优势</strong> </h4>
<ol>
<li><strong>高效性</strong>:
<ul>
<li>稀疏激活和动态路由技术显著减少计算开销。</li>
</ul>
</li>
<li><strong>扩展性</strong>:
<ul>
<li>支持大规模模型的训练与部署。</li>
</ul>
</li>
<li><strong>灵活性</strong>:
<ul>
<li>适用于多种任务和领域，具有广泛的应用潜力。</li>
</ul>
</li>
<li><strong>资源节约</strong>:
<ul>
<li>在低资源环境下仍能高效运行，降低硬件需求。</li>
</ul>
</li>
</ol>
<h4 id="总结-2"><strong>总结</strong> </h4>
<p>DeepSeekMoE 是一种创新的基于 MoE 的深度学习架构，通过稀疏激活、动态路由和分布式训练等技术，实现了高效、灵活和可扩展的模型设计。它特别适合处理大规模任务和低资源环境，在 NLP、CV 和推荐系统等领域具有广泛的应用前景。</p>
<h3 id="代码详解-2">代码详解 </h3>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">MoE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Mixture-of-Experts (MoE) module.

    Attributes:
        dim (int): Dimensionality of input features.
        n_routed_experts (int): Total number of experts in the model.
        n_local_experts (int): Number of experts handled locally in distributed systems.
        n_activated_experts (int): Number of experts activated for each input.
        gate (nn.Module): Gating mechanism to route inputs to experts.
        experts (nn.ModuleList): List of expert modules.
        shared_experts (nn.Module): Shared experts applied to all inputs.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Initializes the MoE module.

        Args:
            args (ModelArgs): Model arguments containing MoE parameters.
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim  <span class="token comment"># 输入特征的维度</span>
        <span class="token comment"># 确保路由专家的数量可以被 world_size 整除</span>
        <span class="token keyword keyword-assert">assert</span> args<span class="token punctuation">.</span>n_routed_experts <span class="token operator">%</span> world_size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Number of experts must be divisible by world size (world_size=</span><span class="token interpolation"><span class="token punctuation">{</span>world_size<span class="token punctuation">}</span></span><span class="token string">)"</span></span>
        self<span class="token punctuation">.</span>n_routed_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_routed_experts  <span class="token comment"># 路由专家的总数</span>
        self<span class="token punctuation">.</span>n_local_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_routed_experts <span class="token operator">//</span> world_size  <span class="token comment"># 每个设备上的本地专家数量</span>
        self<span class="token punctuation">.</span>n_activated_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_activated_experts  <span class="token comment"># 每个样本激活的专家数量</span>
        self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">=</span> rank <span class="token operator">*</span> self<span class="token punctuation">.</span>n_local_experts  <span class="token comment"># 当前设备上专家的起始索引</span>
        self<span class="token punctuation">.</span>experts_end_idx <span class="token operator">=</span> self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">+</span> self<span class="token punctuation">.</span>n_local_experts  <span class="token comment"># 当前设备上专家的结束索引</span>
        self<span class="token punctuation">.</span>gate <span class="token operator">=</span> Gate<span class="token punctuation">(</span>args<span class="token punctuation">)</span>  <span class="token comment"># 门控模块，用于决定输入分配给哪些专家</span>
        <span class="token comment"># 初始化专家列表，仅当前设备上的专家被实例化</span>
        self<span class="token punctuation">.</span>experts <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>Expert<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>moe_inter_dim<span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">&lt;=</span> i <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>experts_end_idx <span class="token keyword keyword-else">else</span> <span class="token boolean">None</span>
                                      <span class="token keyword keyword-for">for</span> i <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>shared_experts <span class="token operator">=</span> MLP<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>n_shared_experts <span class="token operator">*</span> args<span class="token punctuation">.</span>moe_inter_dim<span class="token punctuation">)</span>  <span class="token comment"># 共享专家模块</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for the MoE module.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after expert routing and computation.
        """</span>
        shape <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 保存输入张量的原始形状</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>  <span class="token comment"># 将输入张量展平为二维张量，便于处理</span>
        weights<span class="token punctuation">,</span> indices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 通过门控模块获取权重和路由索引</span>
        y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 初始化输出张量</span>
        counts <span class="token operator">=</span> torch<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>indices<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minlength<span class="token operator">=</span>self<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 统计每个专家被分配到的样本数量</span>
        <span class="token comment"># 遍历当前设备上的专家，计算输出</span>
        <span class="token keyword keyword-for">for</span> i <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>experts_start_idx<span class="token punctuation">,</span> self<span class="token punctuation">.</span>experts_end_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword keyword-if">if</span> counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment"># 如果当前专家未被分配到任何样本，则跳过</span>
                <span class="token keyword keyword-continue">continue</span>
            expert <span class="token operator">=</span> self<span class="token punctuation">.</span>experts<span class="token punctuation">[</span>i<span class="token punctuation">]</span>  <span class="token comment"># 获取当前专家</span>
            idx<span class="token punctuation">,</span> top <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>indices <span class="token operator">==</span> i<span class="token punctuation">)</span>  <span class="token comment"># 获取分配到当前专家的样本索引</span>
            y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+=</span> expert<span class="token punctuation">(</span>x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> weights<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> top<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># 计算当前专家的输出并加权累加</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>shared_experts<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 计算共享专家的输出</span>
        <span class="token keyword keyword-if">if</span> world_size <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 如果使用多设备，则对输出进行全局求和</span>
        <span class="token keyword keyword-return">return</span> <span class="token punctuation">(</span>y <span class="token operator">+</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>  <span class="token comment"># 合并专家输出并将输出恢复为原始形状并返回</span>

<span class="token keyword keyword-class">class</span> <span class="token class-name">Expert</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Expert layer for Mixture-of-Experts (MoE) models.

    Attributes:
        w1 (nn.Module): Linear layer for input-to-hidden transformation.
        w2 (nn.Module): Linear layer for hidden-to-output transformation.
        w3 (nn.Module): Additional linear layer for feature transformation.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> inter_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Initializes the Expert layer.

        Args:
            dim (int): Input and output dimensionality.
            inter_dim (int): Hidden layer dimensionality.
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>  <span class="token comment"># 输入到隐藏层的线性变换</span>
        self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>  <span class="token comment"># 隐藏层到输出层的线性变换</span>
        self<span class="token punctuation">.</span>w3 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>  <span class="token comment"># 输入到隐藏层的额外线性变换</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for the Expert layer.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after expert computation.
        """</span>
        <span class="token comment"># 计算 w1(x) 并使用 SiLU 激活函数【x * torch.sigmoid(x)】(替换ReLU)，然后与 w3(x) 逐元素相乘</span>
        hidden <span class="token operator">=</span> F<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>w3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 将结果通过 w2 线性变换得到最终输出</span>
        <span class="token keyword keyword-return">return</span> self<span class="token punctuation">.</span>w2<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>

<span class="token keyword keyword-class">class</span> <span class="token class-name">Gate</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.

    Attributes:
        dim (int): Dimensionality of input features.
        topk (int): Number of top experts activated for each input.
        n_groups (int): Number of groups for routing.
        topk_groups (int): Number of groups to route inputs to.
        score_func (str): Scoring function ('softmax' or 'sigmoid').
        route_scale (float): Scaling factor for routing weights.
        weight (torch.nn.Parameter): Learnable weights for the gate.
        bias (Optional[torch.nn.Parameter]): Optional bias term for the gate.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Initializes the Gate module.

        Args:
            args (ModelArgs): Model arguments containing gating parameters.
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim  <span class="token comment"># 输入特征的维度</span>
        self<span class="token punctuation">.</span>topk <span class="token operator">=</span> args<span class="token punctuation">.</span>n_activated_experts  <span class="token comment"># 每个输入激活的专家数量</span>
        self<span class="token punctuation">.</span>n_groups <span class="token operator">=</span> args<span class="token punctuation">.</span>n_expert_groups  <span class="token comment"># 路由分组的数量</span>
        self<span class="token punctuation">.</span>topk_groups <span class="token operator">=</span> args<span class="token punctuation">.</span>n_limited_groups  <span class="token comment"># 每个输入路由到的分组数量</span>
        self<span class="token punctuation">.</span>score_func <span class="token operator">=</span> args<span class="token punctuation">.</span>score_func  <span class="token comment"># 评分函数类型（'softmax' 或 'sigmoid'）</span>
        self<span class="token punctuation">.</span>route_scale <span class="token operator">=</span> args<span class="token punctuation">.</span>route_scale  <span class="token comment"># 路由权重的缩放因子</span>
        <span class="token comment"># 可学习的权重矩阵，形状为 (n_routed_experts, dim)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">,</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 可选的偏置项，仅在特定条件下使用</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>dim <span class="token operator">==</span> <span class="token number">7168</span> <span class="token keyword keyword-else">else</span> <span class="token boolean">None</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for the gating mechanism.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: 路由权重和选中的专家索引。
        """</span>
        <span class="token comment"># 计算输入 x 与权重矩阵的线性变换得分</span>
        scores <span class="token operator">=</span> linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token comment"># 根据评分函数类型对得分进行归一化</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"softmax"</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>  <span class="token comment"># 使用 softmax 归一化</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 使用 sigmoid 归一化</span>
        original_scores <span class="token operator">=</span> scores  <span class="token comment"># 保存原始的得分</span>
        <span class="token comment"># 如果存在偏置项，将其加到得分上</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword keyword-is">is</span> <span class="token keyword keyword-not">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token comment"># 如果分组数量大于 1，则进行分组路由</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>n_groups <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 将得分 reshape 为 (batch_size, n_groups, -1)</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_groups<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 如果没有偏置项，计算每个组的最大得分</span>
            <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword keyword-is">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                group_scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>amax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 如果有偏置项，计算每个组的 top-2 得分之和</span>
            <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
                group_scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 选择得分最高的 topk_groups 个组</span>
            indices <span class="token operator">=</span> group_scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>topk_groups<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token comment"># 创建掩码，保留选中的组</span>
            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>scores<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> indices<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token comment"># 根据掩码更新得分，并展平为 (batch_size, n_groups * -1)</span>
            scores <span class="token operator">=</span> <span class="token punctuation">(</span>scores <span class="token operator">*</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 选择得分最高的 topk 个专家</span>
        indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>topk<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 从原始得分中提取选中的专家的权重</span>
        weights <span class="token operator">=</span> original_scores<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
        <span class="token comment"># 如果使用 sigmoid，对权重进行归一化</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>
            weights <span class="token operator">/=</span> weights<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 应用路由缩放因子</span>
        weights <span class="token operator">*=</span> self<span class="token punctuation">.</span>route_scale
        <span class="token comment"># 返回权重和专家索引，确保权重类型与输入一致</span>
        <span class="token keyword keyword-return">return</span> weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> indices
</code></pre><h2 id="关于-multi-token-prediction">关于 Multi-Token Prediction </h2>
<p align="center">
  <img src="file:///c:\Xiuqin\Code\DeepSeek\DeepSeek-V3-main\figures\DeepSeek V3 Multi-Token Prediction.png">
</p>
<h3 id="结果生成">结果生成 </h3>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>
    model<span class="token punctuation">:</span> Transformer<span class="token punctuation">,</span>
    prompt_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    max_new_tokens<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    eos_id<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    temperature<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1.0</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    基于给定的 prompt tokens，使用指定的 Transformer 模型生成新的 tokens。

    Args:
        model (Transformer): 用于生成 tokens 的 Transformer 模型。
        prompt_tokens (List[List[int]]): 包含每个序列的 prompt tokens 的列表。
        max_new_tokens (int): 生成的新 tokens 的最大数量。
        eos_id (int): 序列结束符（End-of-Sequence, EOS）的 token ID。
        temperature (float, optional): 采样温度，控制生成的多样性。默认值为 1.0。

    Returns:
        List[List[int]]: 包含每个序列生成 tokens 的列表。
    """</span>
    <span class="token comment"># 计算每个 prompt 的长度</span>
    prompt_lens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token keyword keyword-for">for</span> t <span class="token keyword keyword-in">in</span> prompt_tokens<span class="token punctuation">]</span>
    <span class="token comment"># 检查 prompt 长度是否超过模型的最大序列长度，如果超过则抛出异常</span>
    <span class="token keyword keyword-assert">assert</span> <span class="token builtin">max</span><span class="token punctuation">(</span>prompt_lens<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> model<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Prompt length exceeds model maximum sequence length (max_seq_len=</span><span class="token interpolation"><span class="token punctuation">{</span>model<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">}</span></span><span class="token string">)"</span></span>
    <span class="token comment"># 计算总长度，即 prompt 长度与新生成 tokens 长度的总和，但不超出模型最大序列长度</span>
    total_len <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> max_new_tokens <span class="token operator">+</span> <span class="token builtin">max</span><span class="token punctuation">(</span>prompt_lens<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 创建一个形状为 (batch_size, total_len) 的 tokens 张量，初始值为 -1</span>
    tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>prompt_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> total_len<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
    <span class="token comment"># 将每个 prompt tokens 填充到 tokens 张量的对应位置</span>
    <span class="token keyword keyword-for">for</span> i<span class="token punctuation">,</span> t <span class="token keyword keyword-in">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>prompt_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        tokens<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>t<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>

    <span class="token comment"># 记录前一个生成位置的索引</span>
    prev_pos <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># 布尔张量，表示每个序列是否已完成生成（即是否遇到 eos_id）</span>
    finished <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompt_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
    <span class="token comment"># 布尔张量，表示 tokens 张量中哪些位置是 prompt 部分</span>
    prompt_mask <span class="token operator">=</span> tokens <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span>

    <span class="token comment"># 从 min(prompt_lens) 到 total_len 进行循环，逐位置生成新 token</span>
    <span class="token keyword keyword-for">for</span> cur_pos <span class="token keyword keyword-in">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span>prompt_lens<span class="token punctuation">)</span><span class="token punctuation">,</span> total_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用模型计算当前 tokens 的 logits</span>
        logits <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> prev_pos<span class="token punctuation">:</span>cur_pos<span class="token punctuation">]</span><span class="token punctuation">,</span> prev_pos<span class="token punctuation">)</span>
        <span class="token comment"># 根据 temperature 采样下一个 token，temperature &gt; 0 时使用 sample 函数，否则取概率最大的 token</span>
        <span class="token keyword keyword-if">if</span> temperature <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            next_token <span class="token operator">=</span> sample<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> temperature<span class="token punctuation">)</span>
        <span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
            next_token <span class="token operator">=</span> logits<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 确保 prompt 部分的 token 不会被覆盖</span>
        next_token <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>prompt_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> cur_pos<span class="token punctuation">]</span><span class="token punctuation">,</span> tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> cur_pos<span class="token punctuation">]</span><span class="token punctuation">,</span> next_token<span class="token punctuation">)</span>
        <span class="token comment"># 更新 tokens 张量</span>
        tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> cur_pos<span class="token punctuation">]</span> <span class="token operator">=</span> next_token
        <span class="token comment"># 检查是否遇到 eos_id，标记生成完成的序列</span>
        finished <span class="token operator">|</span><span class="token operator">=</span> torch<span class="token punctuation">.</span>logical_and<span class="token punctuation">(</span><span class="token operator">~</span>prompt_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> cur_pos<span class="token punctuation">]</span><span class="token punctuation">,</span> next_token <span class="token operator">==</span> eos_id<span class="token punctuation">)</span>
        <span class="token comment"># 更新前一个生成位置的索引</span>
        prev_pos <span class="token operator">=</span> cur_pos
        <span class="token comment"># 如果所有序列都已生成完成，则提前结束循环</span>
        <span class="token keyword keyword-if">if</span> finished<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword keyword-break">break</span>

    <span class="token comment"># 提取每个序列生成的新 tokens</span>
    completion_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword keyword-for">for</span> i<span class="token punctuation">,</span> toks <span class="token keyword keyword-in">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tokens<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 从 tokens 张量中提取生成的 tokens</span>
        toks <span class="token operator">=</span> toks<span class="token punctuation">[</span>prompt_lens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>prompt_lens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span>max_new_tokens<span class="token punctuation">]</span>
        <span class="token comment"># 如果遇到 eos_id，截断生成结果</span>
        <span class="token keyword keyword-if">if</span> eos_id <span class="token keyword keyword-in">in</span> toks<span class="token punctuation">:</span>
            toks <span class="token operator">=</span> toks<span class="token punctuation">[</span><span class="token punctuation">:</span>toks<span class="token punctuation">.</span>index<span class="token punctuation">(</span>eos_id<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token comment"># 将生成结果添加到列表中</span>
        completion_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>toks<span class="token punctuation">)</span>

    <span class="token comment"># 返回所有序列的生成结果</span>
    <span class="token keyword keyword-return">return</span> completion_tokens
</code></pre><h3 id="结果采样">结果采样 </h3>
<p>这段代码实现了一个带有 <strong>温度缩放（Temperature Scaling）</strong> 的采样函数，通常用于从模型的输出 logits 中采样一个 token。它常用于生成模型（如语言模型或图像生成模型）中，以控制生成结果的多样性和随机性。下面我们逐步解释代码的每一部分。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>logits<span class="token punctuation">,</span> temperature<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Samples a token from the logits using temperature scaling.

    Args:
        logits (torch.Tensor): The logits tensor for token predictions.
        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.

    Returns:
        torch.Tensor: The sampled token.
    """</span>
    logits <span class="token operator">=</span> logits <span class="token operator">/</span> <span class="token builtin">max</span><span class="token punctuation">(</span>temperature<span class="token punctuation">,</span> <span class="token number">1e-5</span><span class="token punctuation">)</span>
    probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword keyword-return">return</span> probs<span class="token punctuation">.</span>div_<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">.</span>exponential_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><h4 id="函数功能"><strong>函数功能</strong> </h4>
<p><code>sample</code> 函数的作用是根据输入的 logits，通过温度缩放和随机采样，得到一个 token 索引。</p>
<h4 id="参数说明"><strong>参数说明</strong> </h4>
<ul>
<li><code>logits</code> (<code>torch.Tensor</code>): 模型的输出 logits，形状通常为 <code>(batch_size, vocab_size)</code> 或 <code>(vocab_size,)</code>，表示每个 token 的未归一化分数。</li>
<li><code>temperature</code> (<code>float</code>, 默认值为 1.0): 温度参数，用于调整 logits 的分布。温度越高，采样结果越随机；温度越低，采样结果越倾向于概率最高的 token。</li>
</ul>
<h4 id="代码解释"><strong>代码解释</strong> </h4>
<ul>
<li>
<ol>
<li>温度缩放</li>
</ol>
</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>logits <span class="token operator">=</span> logits <span class="token operator">/</span> <span class="token builtin">max</span><span class="token punctuation">(</span>temperature<span class="token punctuation">,</span> <span class="token number">1e-5</span><span class="token punctuation">)</span>
</code></pre><ul>
<li>将 logits 除以温度值 <code>temperature</code>。</li>
<li><code>max(temperature, 1e-5)</code> 确保温度不会为 0 或负数，否则会导致数值不稳定（分母过小可能导致 logits 值过大，甚至溢出）。</li>
<li>当 <code>temperature &gt; 1</code> 时，logits 的分布变得更平坦，采样结果更随机。</li>
<li>当 <code>temperature &lt; 1</code> 时，logits 的分布更尖锐，采样结果更倾向于概率最高的 token。</li>
</ul>
<ul>
<li>
<ol start="2">
<li>计算概率分布</li>
</ol>
</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><ul>
<li>使用 <code>softmax</code> 函数将缩放后的 logits 转换为概率分布。</li>
<li><code>dim=-1</code> 表示在最后一个维度（通常是 token 的维度）上进行 softmax。</li>
</ul>
<ul>
<li>
<ol start="3">
<li>Gumbel-Max Trick 采样</li>
</ol>
</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code>probs<span class="token punctuation">.</span>div_<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">.</span>exponential_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><ul>
<li>这里实现了 <strong>Gumbel-Max Trick</strong>，一种从离散分布中采样的方法。</li>
<li>步骤如下：
<ol>
<li>生成服从指数分布 <code>Exponential(1)</code> 的随机噪声：<code>torch.empty_like(probs).exponential_(1)</code>。</li>
<li>将概率分布 <code>probs</code> 除以这些随机噪声：<code>probs.div_(...)</code>。</li>
<li>对处理后的值取 <code>argmax</code>，即找到最大值的索引。这一步实现了从分布中采样一个 token。</li>
</ol>
</li>
<li>使用 Gumbel-Max Trick 可以在保证采样随机性的同时，避免直接使用 <code>torch.multinomial</code> 函数的计算开销。</li>
</ul>
<ul>
<li>返回值</li>
</ul>
<ul>
<li><code>torch.Tensor</code>: 采样得到的 token 的索引，形状为 <code>(batch_size,)</code> 或 <code>()</code>（标量）。</li>
</ul>
<h4 id="温度参数的作用"><strong>温度参数的作用</strong> </h4>
<ul>
<li><strong><code>temperature = 1.0</code></strong>: 保持 logits 的原始分布，采样结果直接基于 softmax 后的概率。</li>
<li><strong><code>temperature &gt; 1.0</code></strong>: 使 logits 的分布更平坦，增加随机性，生成结果更多样。</li>
<li><strong><code>temperature &lt; 1.0</code></strong>: 使 logits 的分布更尖锐，减少随机性，生成结果更倾向于概率最高的 token。</li>
</ul>
<h4 id="总结-3"><strong>总结</strong> </h4>
<p>该代码实现了基于温度缩放的 token 采样方法，通过调整温度参数 <code>temperature</code>，可以控制采样结果的随机性和多样性。Gumbel-Max Trick 的使用使得采样过程更加高效和稳定。</p>
<h2 id="关于训练成本">关于训练成本 </h2>
<p align="center">
  <img src="file:///c:\Xiuqin\Code\DeepSeek\DeepSeek-V3-main\figures\DeepSeek V3 成本.png">
</p>
从论文中的公布细节可以得到它的训练成本估算：
<ul>
<li>以 H800 GPU 小时为单位。H800 GPU 的租赁价格假定为每小时 2 美元。</li>
<li>训练分为三个阶段：预训练、上下文扩展和后期训练：</li>
<li>预训练：使用了 2664K（266.4 万）GPU 小时，成本约为 532.8 万美元。</li>
<li>上下文扩展：使用了 119K（11.9 万）GPU 小时，成本约为 23.8 万美元。</li>
<li>后期训练：使用了 5K GPU 小时，成本约为 1,000 美元。</li>
<li>总成本：2788K（278.8 万）GPU 小时，总费用为 557.6 万美元。</li>
</ul>
<p>比起动辄几百亿人民币都训练不出来一个好用的大模型，DeepSeek V3的训练简直颠覆了大家的想象。可能有如下原因：</p>
<h3 id="浮点精度降低">浮点精度降低 </h3>
<p>这里训练这么省钱当然主要是因为该模型原生就是FP8，还有在模型架构上做了一些优化导致模型训练成本很低。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-def">def</span> <span class="token function">fp8_gemm</span><span class="token punctuation">(</span>a<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> a_s<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> b<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> b_s<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Perform a matrix multiplication using FP8 precision.

    Args:
        a (torch.Tensor): The first input matrix, must be contiguous.
        a_s (torch.Tensor): The scaling factor for the first input matrix, must be contiguous.
        b (torch.Tensor): The second input matrix, must be contiguous.
        b_s (torch.Tensor): The scaling factor for the second input matrix, must be contiguous.

    Returns:
        torch.Tensor: The result of the matrix multiplication.
    """</span>
    <span class="token keyword keyword-assert">assert</span> a<span class="token punctuation">.</span>is_contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-and">and</span> b<span class="token punctuation">.</span>is_contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'Input tensors must be contiguous'</span>
    <span class="token keyword keyword-assert">assert</span> a_s<span class="token punctuation">.</span>is_contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword keyword-and">and</span> b_s<span class="token punctuation">.</span>is_contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'Scaling factor tensors must be contiguous'</span>
    K <span class="token operator">=</span> a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    M <span class="token operator">=</span> a<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">//</span> K
    N <span class="token operator">=</span> b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    c <span class="token operator">=</span> a<span class="token punctuation">.</span>new_empty<span class="token punctuation">(</span><span class="token operator">*</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> N<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    grid <span class="token operator">=</span> <span class="token keyword keyword-lambda">lambda</span> META<span class="token punctuation">:</span> <span class="token punctuation">(</span>triton<span class="token punctuation">.</span>cdiv<span class="token punctuation">(</span>M<span class="token punctuation">,</span> META<span class="token punctuation">[</span><span class="token string">'BLOCK_SIZE_M'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> triton<span class="token punctuation">.</span>cdiv<span class="token punctuation">(</span>N<span class="token punctuation">,</span> META<span class="token punctuation">[</span><span class="token string">'BLOCK_SIZE_N'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    fp8_gemm_kernel<span class="token punctuation">[</span>grid<span class="token punctuation">]</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> a_s<span class="token punctuation">,</span> b_s<span class="token punctuation">,</span> M<span class="token punctuation">,</span> N<span class="token punctuation">,</span> K<span class="token punctuation">)</span>
    <span class="token keyword keyword-return">return</span> c
</code></pre><h3 id="注意力机制优化">注意力机制优化 </h3>
<h4 id="引入-latent-features">引入 Latent Features </h4>
<p>DeepSeek V3除了使用了FP8之外，还有一些其他的模型细节。比如它继续采用了多头潜在注意力（MLA）来实现高效推理。它在传统多头注意力机制（Multi-Head Attention）的基础上，引入了潜在【隐】特征（Latent Features）概念，进一步提高了对复杂关系的建模能力。</p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
<span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>wq_a <span class="token operator">=</span> Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>q_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>wq_b <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
</code></pre><p>也就是先把token的特征压缩成一个小维度的latent vector，然后再通过一些简单的变换把它扩展到各个头需要的Key和Value空间。对于一些重要的信息，比如旋转位置编码RoPE，会进行单独处理，这样网络仍然可以保留时间和位置的信息。</p>
<h4 id="缓存-key-value">缓存 Key-Value </h4>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-if">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"k_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"v_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token keyword keyword-else">else</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"kv_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"pe_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre><h3 id="moe-优化">MoE 优化 </h3>
<p>在MOE架构中，引入了路由专家 (Routed Experts) 和共享专家 (Shared Experts) 。主要是用来激活那些参数需要被更新。</p>
<p>路由专家中主要是用来选择参数进行激活。对于每个输入的token，只有一部分路由专家会被选中来参与计算。这个选择过程是由一个门控机制决定的，比如DeepSeekMoE中用的那种根据亲和度分数来选的Top-K方式。</p>
<p>而共享专家始终参与所有输入的处理。无论输入是什么，所有共享专家都会贡献它们的力量。</p>
<h3 id="mtp-技术">MTP 技术 </h3>
<p>还用到了一个MTP（多个tokens预测）技术，MTP的核心理念在于训练时，模型不仅要预测下一个token（就像传统语言模型那样），还要同时预测序列后面的几个token。这样一来，模型就能获得更丰富的训练信息，有助于它更深入地理解上下文以及长距离的依赖关系。</p>
<h3 id="并行计算">并行计算 </h3>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token keyword keyword-class">class</span> <span class="token class-name">ColumnParallelLinear</span><span class="token punctuation">(</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Linear layer with column parallelism, splitting output features across distributed processes.

    Args:
        in_features (int): Number of input features.
        out_features (int): Total number of output features.
        bias (bool): Whether to include a bias term. Defaults to False.
        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword keyword-assert">assert</span> out_features <span class="token operator">%</span> world_size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Output features must be divisible by world size (world_size=</span><span class="token interpolation"><span class="token punctuation">{</span>world_size<span class="token punctuation">}</span></span><span class="token string">)"</span></span>
        self<span class="token punctuation">.</span>part_out_features <span class="token operator">=</span> out_features <span class="token operator">//</span> world_size
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>part_out_features<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for column parallel linear layer.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Transformed tensor with column-parallel computation.
        """</span>
        y <span class="token operator">=</span> linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
        <span class="token keyword keyword-return">return</span> y


<span class="token keyword keyword-class">class</span> <span class="token class-name">RowParallelLinear</span><span class="token punctuation">(</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Linear layer with row parallelism, splitting input features across distributed processes.

    Args:
        in_features (int): Total number of input features.
        out_features (int): Number of output features.
        bias (bool): Whether to include a bias term. Defaults to False.
        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.
    """</span>
    <span class="token keyword keyword-def">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword keyword-assert">assert</span> in_features <span class="token operator">%</span> world_size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Input features must be divisible by world size (world_size=</span><span class="token interpolation"><span class="token punctuation">{</span>world_size<span class="token punctuation">}</span></span><span class="token string">)"</span></span>
        self<span class="token punctuation">.</span>part_in_features <span class="token operator">=</span> in_features <span class="token operator">//</span> world_size
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">.</span>part_in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span>

    <span class="token keyword keyword-def">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass for row parallel linear layer.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Transformed tensor with row-parallel computation.
        """</span>
        y <span class="token operator">=</span> linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword keyword-if">if</span> world_size <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        <span class="token keyword keyword-if">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword keyword-is">is</span> <span class="token keyword keyword-not">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            y <span class="token operator">+=</span> self<span class="token punctuation">.</span>bias
        <span class="token keyword keyword-return">return</span> y
</code></pre>
      </div>
      
      
    </body>
    
    
    
    
    
  </html>